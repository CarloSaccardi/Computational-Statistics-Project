---
title: "Costruzione di un modello lineare robusto relativo ad un e-commerce francese"
author: "Martina Chiesa 837484, Carlo Saccardi 839641, Davide Valoti 846737"
date: "20/11/2020"
output: word_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Il dataset scelto contiene informazioni relative ad un e-commerce francese di successo, presente in diversi Paesi, basato sul modello economico C2C (customer to customer), in cui ogni utente è sia venditore, sia acquirente.
```{r}
setwd("C:\\Users\\Carlo\\Desktop\\statistica_computazionale\\serie storiche")
d <- read.csv("dati.csv", sep=","
                  , dec = ".",  
                  stringsAsFactors=TRUE, na.strings=c("NA","NaN",-1))
summary(d)
```

Il dataset è composto da 98913 righe e 24 colonne.
Ciascuna riga corrisponde a un utente registrato, quindi, nel file non sono presenti clienti non registrati, infatti la variabile *type* presenta una sola modalità. Ci sono alcune variabili quantative e altre qualitative: *identifierHash* comprende i codici identificativi corrispondenti a ciascun utente; *type* indica la tipologia di cliente; *country* e *countryCode* corrispondono a nome e codice ISO del Paese di appartenenza dell'utente; *language* si riferisce alla lingua selezionata come preferita tra le cinque opzioni proposte; *socialNbFollowers* numero di utenti iscritti all'attività di questo user; *socialNbFollows* numero di utenti seguiti dallo user; *socialProductsLiked* numero di prodotti graditi dall'utente; *productsListed* numero di prodotti attualmente non venduti ma caricati dall'utente; *productsSold* numero di prodotti venduti; *productsPassRate* percentuale di prodotti la cui descrizione è coerente col bene offerto; *productsWished* numero di prodotti aggiunti alla lista dei desideri; *productsBought* numero di prodotti acquistati; *gender* genere dell'utente; *civilityTitle* e *civilityGenderId* indica lo stato civile e la rispettiva codifica in numeri da 1 a 3; *hasAnyApp* indica se l'utente ha mai utilizzato l'app ufficiale dello store, in caso affermativo, se è la versione Android *hasAndroidApp* o Ios *hasIosApp*; *hasProfilePicture* indica se è presente l'immagine del profilo; *daysSinceLastLogin* è il numero di giorni trascorsi dall'ultimo login; *seniority*, *seniorityAsMonths*, *seniorityAsYears* corrispondono rispettivamente al numero di giorni, mesi e anni decorsi dalla registrazione.

Tramite la funzione summary si ottengono alcune delle statistiche descrittive univariate relative ad ogni variabile.
Notiamo che non sono presenti valori negativi nel dataset, infatti per tutte le variabili il valore minimo è pari o superiore a 0. I valori -1 presenti nel file sono stati letti come mancanti, durante l'importazione del dataset.
Il maggior numero di utenti è francese, come ci aspettavamo, dato che l'e-commerce è nato proprio in questo Stato, ma la lingua più ricorrente, tra le 5 presenti, è l'inglese.
Rilevante è inoltre la presenza di valori mancanti per le variabili *proudctsListed* e *productsWished*.
Dando uno sguardo alla variabile dipendente scelta (*productsSold*), si osserva che il valore medio assegnato a questa è 0.12. 
Dal terzo quartile si constata che, almeno il 75% degli utenti non ha venduto nessun prodotto. Il campo di variazione è ampio poichè, il numero massimo di prodotti venduti è 174, rispetto al minimo 0. 

L'intero file contiene dati relativi agli utenti registrati, i quali possono sia aver venduto, che non. Vogliamo quindi indagare la differenza tra queste due categorie di utenti.
```{r}
table(d$productsSold>0)
```

Coloro che hanno venduto almeno un prodotto sono 2036 e corrispondono solo al 2% circa di tutti gli utenti presenti nel dataset.

Procediamo quindi con la creazione di un nuovo dataset che contiene unicamente questi ultimi, ovvero i venditori.
```{r}
d0 <- d[d$productsSold>0,]
summary(d0$productsSold)
```

Il dataset ridotto presenta lo stesso numero di variabili di quello completo.
I cambiamenti della variabile dipendente sono evidenti, infatti, ora dalla mediana si osserva che la metà degli utenti ha venduto al massimo 2 prodotti. 
Un quarto ha venduto un solo prodotto; un altro quarto, invece 3, 4 o 5 prodotti (terzo quartile). La media del numero di prodotti venduti è pari quasi a 6 prodotti.

```{r}
library(funModeling)
library(dplyr)
status = df_status(d0, print_results=F)
status
```

Tramite la funzione df_status si osservano le quantità in termini assoluti e percentuali del numero di 0, di NA e di valori unici presenti nel dataset per ogni variabile. 
Le due percentuali di valori mancanti (4.96 e 3.19) sono poco elevate, quindi decidiamo di conservare le rispettive variabili nel dataset e procedere successivamente con l'imputazione. 
La variabile *civilityGenderId* assume valore 1, 2 o 3, tuttavia non si tratta di una variabile quantitativa, poichè queste tre cifre sono codifiche dei tre livelli presenti in *civilityTitle*, ovvero 'miss', 'mr', e 'mrs'.
Dunque, procediamo con la correzione, trasformandola in fattore a tre livelli.
```{r}
d0$civilityGenderId <- as.factor(d0$civilityGenderId)
```

Innanzitutto, rimuoviamo la variabile identificativa *identifierHash*, e *type*, poichè presenta un solo livello, quindi si tratta di una variabile non discriminante.
```{r}
d0$identifierHash = NULL
d0$type = NULL
```

## NA analysis
```{r}
library(VIM)
missingness<- aggr(d0, col=c('navyblue','yellow'),numbers=TRUE, sortVars=TRUE,labels=names(d0), cex.axis=.5,gap=2)
```

In giallo si evidenzia la proporzione di dati mancanti e la combinazione con cui questi si presentano. Ci sono righe che hanno NA per entrambe le variabili ed altre che sono caratterizzate da un solo valore mancante. 

Per conferma utilizziamo un'altra tecnica di verifica:
```{r}
sapply(d0, function(x)(sum(is.na(x))))
```

*productsListed* presenta 101 dati mancanti e *productsWished* 65.

Decidiamo di procedere con la mice imputation:
```{r}
library(dplyr)
numeric <- d0%>% dplyr::select_if(is.numeric)
covariate <- numeric[, -c(5)]
library(mice)
md.pattern(covariate)yy
nrow(na.omit(covariate))
```

Consideriamo solo le variabili numeriche escludendo quella dipendente e osserviamo che ci sono 1920 righe complete, 15 presentano valori mancanti solo per la variabile *productsListed*, 51 per *productsWished* e 50 per entrambe. Quindi sarà necessario stimare 166 valori. 

Scegliamo di utilizzare il metodo pmm (predictive mean matching) con 5 ripetizioni. Questo metodo stima un modello per ciascun campione (per noi 5), considerando come variabile target quella incompleta. Ora risulta possibile ricavare, per ogni dato mancante, 5 previsioni.
Una volta calcolata la media tra queste cinque previsioni, vengono considerati i 10 valori osservati nel dataset più prossimi a tale media, e ne viene estratto uno casualmente, che diventa il prescelto per imputare l'NA. La tecnica di selezione casuale tra le 10 osservazioni garantisce effetto di randomness.
```{r}
tempData <- mice(covariate, m=5, maxit = 20, meth='pmm', seed=500) 
```

L'imputazione è andata a buon fine, in quanto è stata raggiunta convergenza.

Inserendo nel dataset i valori imputati ne otteniamo uno nuovo e verifichiamo che non siano più presenti dati mancanti.
```{r}
data_imputed <- complete(tempData,1)
sapply(data_imputed, function(x)(sum(is.na(x))))
```

## Collinearità
L'analisi della collinearità è uno step necessario per la costruzione di un modello robusto, dato che variabili correlate comportano problemi di efficienza e non permettono di ottenere stime OLS precise. 

Tramite la funzione corrgram viene proposta una rappresentazione grafica delle correlazioni presenti tra le variabili quantitative considerate. 
```{r}
library(dplyr)
numeric <- data_imputed%>% dplyr::select_if(is.numeric)
require(corrgram)
corrgram(numeric,lower.panel = panel.cor, cex=1, cex.labels = 1)
```

Notiamo come le variabili *seniority*, *senorityAsMonths* e *seniorityAsYears* sono perfettamente correlate, come era lecito aspettarsi. Anche tra le variabili *socialNbFollowers*, *socialNbFollows* e *socialProductsLiked* sono presenti forti correlazioni positive. 

Questa rappresentazione mediante matrice delle collinearità presenta alcuni limiti, 
risulta infatti, di difficile interpretazione quando il numero di variabili è elevato, inoltre, analizza solamente le correlazioni bivariate. 
Procediamo allora con l'analisi tramite altre metodologie, che permettano di considerare anche la presenza di possibile multicollinearità.
Formuliamo un nuovo modello che spiega la variabile risposta solamente tramite le variabili quantitative per poter poi procedere all'analisi di VIF e TOL.
```{r}
numeric_model = lm(d0$productsSold ~ socialNbFollowers + socialNbFollows + 
                   socialProductsLiked + productsListed + productsPassRate + 
                   productsWished + productsBought + daysSinceLastLogin + seniority 
                   + seniorityAsMonths + seniorityAsYears, data = numeric)
library(mctest)
imcdiag(numeric_model)
```

La colonna Klein presenta il valore 1 in corrispondenza di sei variabili, tale numero è sinonimo di collinearità. La tolleranza (TOL) è il valore di 1-R2j dove R2j indica l'indice di determinazione del modello che ha come variabile risposta la j-esima variabile e come covariate le restanti, mentre VIF è l'inverso della tolleranza. 
Questi indici si servono dell'R^2, perchè questo è in grado di esprimere la quota di varianza spiegata della variabile risposta dalle covariate congiuntamente.
Convenzionalemnte, i valori di VIF devono essere inferiori a 5 mentre i valori di TOL superiori a 0,3. Procediamo, quindi, all'eliminazione della variabile che presenta un valore di VIF più elevato, ovvero *seniority*. Segue, poi la stima del nuovo modello. 
```{r}
numeric$seniority = NULL
numeric_model2 = lm(d0$productsSold ~ socialNbFollowers + socialNbFollows + 
                    socialProductsLiked + productsListed + productsPassRate + 
                    productsWished + productsBought + daysSinceLastLogin + 
                    seniorityAsMonths + seniorityAsYears, data = numeric)
imcdiag(numeric_model2)
```

I valori di VIF sono diminuiti, ma rimangono sopra la soglia 5, pertanto procediamo all'eliminazione della variabile *seniorityAsYears* che presenta il valore maggiore.
```{r}
numeric$seniorityAsYears = NULL
numeric_model3 = lm(d0$productsSold ~ socialNbFollowers + socialNbFollows + 
                    socialProductsLiked + productsListed + productsPassRate +
                    productsWished + productsBought + daysSinceLastLogin +
                    seniorityAsMonths, data = numeric)
imcdiag(numeric_model3)
```

Osserviamo valori più contenuti, l'unica variabile che non rispetta le soglie, anche se di poco, è *socialNbFollows* il cui valore del VIF è di poco superiore a 5 e la tolleranza è pari a 0.20 circa, quindi procediamo alla sua eliminazione e riformuliamo il modello.
```{r}
numeric$socialNbFollows = NULL
numeric_model4 = lm(d0$productsSold ~ socialNbFollowers + 
                    socialProductsLiked + productsListed + productsPassRate +
                    productsWished + productsBought + daysSinceLastLogin + 
                    seniorityAsMonths, data = numeric)
imcdiag(numeric_model4)
```

Osserviamo che la colonna Klein non individua più collinearità, i valori di VIF sono tutti inferiori a 5 e quelli di TOL superiori a 0.30. Concludiamo che non risultano ulteriori eliminazioni da compiere e rimangono otto covariate quantitative.

Analizziamo ora le associazioni tra le variabili qualitative. 
Se il chi quadro normalizzato associato a una coppia di variabili presenta un valore superiore a 0.90, allora è presente una forte associazione, quindi escludiamo una tra le due variabili coinvolte.
```{r}
factor <- d0%>% dplyr::select_if(is.factor)
library(plyr)
combos <- combn(ncol(factor),2)
adply(combos, 2, function(x) {
  test <- chisq.test(factor[, x[1]], factor[, x[2]])
  tab  <- table(factor[, x[1]], factor[, x[2]])
  out <- data.frame("Row" = colnames(factor)[x[1]]
                    , "Column" = colnames(factor[x[2]])
                    , "Chi.Square" = round(test$statistic,3)
                    , "df"= test$parameter
                    , "p.value" = round(test$p.value, 3)
                    , "n" = sum(table(factor[,x[1]], factor[,x[2]]))
                    , "Chi.Square norm"  =test$statistic/(sum(table(factor[,x[1]], factor[,x[2]]))* min(length(unique(factor[,x[1]]))-1 , length(unique(factor[,x[2]]))-1)))
return(out)
}) 
```

Notiamo dalla colonna Chi.Square.norm un valore pari a 1 per le coppie di variabili *gender* con *civilityTitle*, *gender* con *civilityGenderId* e *civilityTitle* con *civilityGenderId*. 

```{r}
table(factor$civilityTitle, factor$gender)
table(factor$civilityGenderId, factor$gender)
```

Dalla tabella di contigenza notiamo infatti massima associazione tra queste variabili. Tale risultato era prevedibile, poichè tutte e tre considerano il genere.

Procediamo all'eliminazione delle variabili *gender* e *civilityGenderID* poichè entrambe perfettamente associate alla variabile *civilityTitle*.  
```{r}
factor$civilityGenderId = NULL
factor$gender = NULL
combos <- combn(ncol(factor),2)
adply(combos, 2, function(x) {
  test <- chisq.test(factor[, x[1]], factor[, x[2]])
  tab  <- table(factor[, x[1]], factor[, x[2]])
  out <- data.frame("Row" = colnames(factor)[x[1]]
                    , "Column" = colnames(factor[x[2]])
                    , "Chi.Square" = round(test$statistic,3)
                    , "df"= test$parameter
                    , "p.value" = round(test$p.value, 3)
                    , "n" = sum(table(factor[,x[1]], factor[,x[2]]))
                    , "u1" =length(unique(factor[,x[1]]))-1
                    , "u2" =length(unique(factor[,x[2]]))-1
                    , "nMinu1u2" =sum(table(factor[,x[1]], factor[,x[2]]))* min(length(unique(factor[,x[1]]))-1 , length(unique(factor[,x[2]]))-1) 
                    , "Chi.Square norm"  =test$statistic/(sum(table(factor[,x[1]], factor[,x[2]]))* min(length(unique(factor[,x[1]]))-1 , length(unique(factor[,x[2]]))-1)))
  return(out)
})
```

I chi quadrati normalizzati osservati tra le variabili qualitative rimaste sono tutti inferiori alla soglia 0.90, pertanto non procediamo ad ulteriori eliminazioni.
Dopo l'analisi dei chi quadrati rimangono in considerazione otto variabili qualitative.

## Starting model
Procediamo alla formulazione dello starting model.
```{r}
data_completo <- cbind(d0$productsSold,numeric,factor)
names(data_completo)[names(data_completo) == "d0$productsSold"] <- "productsSold"
starting_model = lm(productsSold ~ socialNbFollowers + socialProductsLiked + 
                      productsListed + productsPassRate + productsWished + 
                      productsBought + daysSinceLastLogin + seniorityAsMonths + 
                      language + civilityTitle + hasAnyApp + hasAndroidApp + hasIosApp
                      + hasProfilePicture + country + countryCode, data =
                      data_completo)
par(mfrow=c(2,2)) 
plot(starting_model)
par(mfrow=c(1,1))
```

Uniamo le variabili qualitative e quantitative rimaste in analisi con la variabile risposta. 
Il modello presenta un R2 aggiustato di circa 0.63 e un elevato numero di coefficienti, molti non significativi. Analizziamo ora i grafici per valutare le assunzioni di robustezza sul modello.
I grafici del modello di partenza presentano diverse problematiche. Notiamo in particolare valori di leverage molto elevati e una situazione di forte eteroschedasticità (terzo grafico).

```{r}
plot(data_completo$productsSold, starting_model$fitted.values)
```

Il grafico dei valori di y vs y fitted presenta una grande concentrazione di punti nella porzione a sinistra del plot. 
La nuvola dei punti non sembra seguire un preciso andamento lineare e sono presenti diverse osservazioni anomale.

## Linearità
Procediamo con l'analisi della linearità per il modello iniziale, dapprima valutando la variabile risposta. L'ipotesi di linearità è un'assunzione molto forte e se non rispettata comporta stime non BLUE, distorte ed inefficienti.
Tramite la funzione boxcox otteniamo il valore di lambda. che minimizza l'SSE (sum of squares errors), a cui dobbiamo elevare la variabile risposta per ottenere la miglior trasformazione di questa.
```{r}
library(MASS)
bc <- boxcox(starting_model, plotit = T)
lambda = bc$x[which.max(bc$y)]
lambda
```

Il valore di lambda risulta essere prossimo a -0.5.

```{r}
model_1 = lm((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked + productsListed
              + productsPassRate + productsWished + productsBought + daysSinceLastLogin +
              seniorityAsMonths + language + civilityTitle + hasAnyApp + hasAndroidApp +
              hasIosApp + hasProfilePicture + country + countryCode, data = data_completo)
par(mfrow=c(2,2)) 
plot(model_1)
par(mfrow=c(1,1))
```

L'R^2 aggiustato di questo modello aumenta rispetto al precedente, tuttavia il nostro intento è quello di migliorare i grafici per garantire l'assunzione di robustezza del modello, senza concentrarci sull'adattamento dei dati.
Notiamo inoltre un cambiamento della variabile più significativa, che risulta essere *productsPassRate* mentre prima della trasformazione era identificata da *socialNbFollowers*.
I grafici appaiono molto differenti rispetto a quelli del modello iniziale. In particolare, si evince un netto miglioramento del terzo grafico relativo ai fitted values che ora sembrano essere più distribuiti e più lineari.

Monitoriamo l'evoluzione del nostro modello anche tramite il grafico di Y vs Yfittati.
```{r}
plot((data_completo$productsSold)^(-0.5), model_1$fitted.values)
```

Anche questo grafico è nettamente migliore del precedente, poichè l'andamento dei dati sembra essersi linearizzato, pur restando alcune osservazioni anomale.

A questo punto dell'analisi, ci concentriamo sulla linearità delle singole covariate, nello specifico operiamo con l'optimal grouping per le variabili qualitative.

Le variabili *country* e *countryCode* presentano un numero elevato di livelli, corrispondenti a 42 Stati, quindi ci serviamo della procedura di optimal grouping per ridurli. 
```{r}
library(factorMerger)
reduce_levels <- mergeFactors(response = data_completo$productsSold, factor = data_completo$countryCode)
plot(reduce_levels, panel = "GIC", title = "", panelGrid = FALSE)
opt_group = cutTree(reduce_levels)
```

Il Merging Path Plot panel mostra la struttura gerarchica delle similarità tra i gruppi. Le stelle indicano quanto sono significative le differenze tra due cluster.
I valori numerici sono i valori della funzione di logverosimiglianza per ogni modello della lista. Invece, il GIC panel mostra il criterio di informazione generalizzato per tutti i modelli presenti nel merging path plot. (default penalty=2 indica il criterio AIC).
I 2036 utenti sono classificati attraverso l'optimal grouping in base alla sigla dello Stato in cinque gruppi di ampiezza differente: il più numeroso comprende 1056 utenti e il più piccolo ne caratterizza solo 6.

Ora procediamo trasformando la variabile in numerica e poi fattoriale.
```{r}
opt_group2 = as.numeric(opt_group)
table(opt_group2)
data_completo$optimal_countrycode = as.factor(opt_group2)
d0$optimal_countrycode = as.factor(opt_group2)
```

Otteniamo quindi *optimal_countrycode*, una variabile ricodificata in livelli identificati con numeri interi da 1 a 5.

```{r}
plot(data_completo$optimal_countrycode, data_completo$productsSold)
```

I boxplot confermano la divisione in gruppi effettuata ed è possibile osservare che i valori delle mediane aumentano in corrispondenza del passaggio al livello successivo. Tale considerazione è in accordo con la procedura di optimal grouping svolta.
Sono presenti anche dei possibili valori anomali, in particolare per i primi tre livelli, ovvero quelli che comprendono più osservazioni.

Sempre nel contesto dell'analisi della linearità, formuliamo un modello gam con la stessa struttura di model_1, seguito da uno analogo, con l'aggiunta di "s" che precedono le variabili quantitative, che, a nostro avviso, potrebbero avere una relazione non lineare con la dipendente.
```{r}
library(mgcv)
model_gam = gam((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked +
                  productsListed + productsPassRate + productsWished + 
                  productsBought + daysSinceLastLogin + seniorityAsMonths + 
                  language + civilityTitle + hasAnyApp + hasAndroidApp + hasIosApp 
                  + hasProfilePicture + optimal_countrycode, data = data_completo)

model_2 = gam((productsSold)^(-0.5) ~ s(socialNbFollowers) + s(socialProductsLiked)
              + s(productsListed) + s(productsPassRate) + s(productsWished) +
              s(productsBought) + daysSinceLastLogin + seniorityAsMonths + language
              + civilityTitle + hasAnyApp + hasAndroidApp + hasIosApp +
              hasProfilePicture + optimal_countrycode, data = data_completo)
summary(model_2)

```

Dal summary del modello ricaviamo la significatività approsimativa delle trasformazioni. Valutiamo in particolare *socialNbFollowers*, *socialProductsLiked*, *productsListed* e *productsPassRate* che presentano p-value significativi.

Eseguiamo ora il test del rapporto di verosomiglianza (Likelihood Ratio Test) per effettuare un confronto tra il modello lineare ottenuto in seguito alla trasformazione box-cox (model_2) e il modello gam appena formulato.
```{r}
anova.gam(model_gam, model_2, test = "LRT")
```

Notiamo che il p-value osservato per la statistica Chi-quadrato è prossimo a 0 pertanto possiamo concludere che il modello gam è significativamente migliore in termini di likelihood.

Analizziamo i plot del modello gam per osservare graficamente se l'andamento delle covariate è lineare o segue un'altra distribuzione. 
I grafici sono ottenuti tramite procedura splines, ovvero il grafico viene diviso in finestre che restano fisse e in ognuna viene stimato il miglior polinomio di grado 3 per i dati presenti.
```{r}
plot(model_2, ylim=c(-1,1))
```

Poniamo particolare attenzione all'andamento delle funzioni in corrispondenza di concentrazione maggiore di trattini, posizionati sull'asse delle ascisse, che indicano le osservazioni del dataset. 
L'unica variabile che presenta un andamento non lineare è *productsPassRate*, quindi formuliamo un nuovo modello che presenta questa variabile anche al secondo e terzo grado.
```{r}
model_3 = gam((productsSold)^(-0.5) ~ s(socialNbFollowers) + s(socialProductsLiked) + 
                s(productsListed) + productsPassRate + I(productsPassRate^2) + 
                I(productsPassRate^3) + s(productsWished) +  s(productsBought) + 
                daysSinceLastLogin + seniorityAsMonths + language + civilityTitle +
                hasAnyApp + hasAndroidApp + hasIosApp + hasProfilePicture + 
                optimal_countrycode, data = data_completo)
```

Il coefficiente di terzo grado risulta significativo, pertanto manteniamo la trasformazione anche nel modello lineare.
```{r}
model_4 = lm((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked + 
               productsListed + productsPassRate + I(productsPassRate^2) + 
               I(productsPassRate^3) + productsWished + productsBought + 
               daysSinceLastLogin + seniorityAsMonths + language + civilityTitle + 
               hasAnyApp + hasAndroidApp + hasIosApp + hasProfilePicture + 
               optimal_countrycode, data = data_completo)
summary(model_4)
par(mfrow=c(2,2)) 
plot(model_4)
par(mfrow=c(1,1))
```

Rispetto a model_1 notiamo un miglioramento di fitting anche se il nostro interesse si focalizza sui grafici. Tuttavia questi non mostrano significativi miglioramenti per quanto riguarda eteroschedasticità e residui. 
Rimaniamo comunque soddisfatti delle trasformazioni efffettuate nell'analisi della linearità, in particolare per i miglioramenti apportati dalla trasformazione boxcox della variabile risposta.

## Model selection
Spesso, alcune variabili inserite nel modello di regressione non sono significativamente associate con la variabile risposta. Se queste vengono incluse, pur essendo poco rilevananti, si rende il modello più complesso del necessario, complicando l'interpretazione degli output. La model selection mira dunque a rimuovere queste variabili, così da ottenere un modello facilmente interpretabile. 
```{r}
d0_nona <- na.omit(d0)
model_5 = lm((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked + 
               productsListed + productsPassRate + I(productsPassRate^2) +
               I(productsPassRate^3) + productsWished + productsBought + 
               daysSinceLastLogin + seniorityAsMonths + language + civilityTitle + 
               hasAnyApp + hasAndroidApp + hasIosApp + hasProfilePicture + 
               optimal_countrycode, data = d0_nona)
```

La funzione di R stepAIC con direzione "both" svolge una strategia di model selection chiamata 'stepwise selection' sul modello di interesse (nel nostro caso model_5). La stepwise selection ci permette di ottenere un modello parsimonioso, ovvero semplice e allo stesso tempo performante per prevedere la y con accuratezza.  
```{r}
library(MASS)
step <- stepAIC(model_5, direction="both")
```
 
```{r}
model_aic = lm((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked + 
                 productsListed + productsPassRate + I(productsPassRate^2) + 
                 I(productsPassRate^3) + daysSinceLastLogin + civilityTitle + 
                 optimal_countrycode, data=d0_nona)
summary(model_5)
summary(model_aic)
```

Osservando i due summary, notiamo che, grazie alla stepwise selection, riusciamo ad ottenere un modello semplice e performante, senza perdite in termini di fitting. 

In alternativa eseguiamo un altro stepwise basandoci sull'indice SBC, molto simile ad AIC, ma più severo per i modelli con un numero di covariate maggiore. Ci aspettiamo dunque un modello più semplice, con meno covariate.
```{r}
step2 <- stepAIC(model_5, direction="both", k = log(nrow(data_completo)))
```

Il modello rispetta le aspettative, infatti presenta due covariate in meno (*civilityTitle* e *optimalcountryCode*) rispetto a quello ottenuto con l'AIC.

```{r}
model_sbc = lm((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked + 
                 productsListed + productsPassRate + I(productsPassRate^2) + 
                 I(productsPassRate^3) + daysSinceLastLogin, data=d0_nona)
summary(model_sbc)
```

Poichè la bontà dei due modelli (model_aic e model_sbc) non differeisce in modo significativo, scegliamo di mantenere il modello più semplice per le successive analisi.

Una volta svolta la model selection, rifittiamo il modello scelto sul dataset di partenza. Questo passagio è importante poichè, in certi casi, il modello più parsimonioso viene stimato su un numero di osservazioni maggiore rispetto a quello su cui è stato stimato il modello di partenza. 
```{r}
model_sbc2 = lm((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked + 
                productsListed + productsPassRate + I(productsPassRate^2) + 
                I(productsPassRate^3) + daysSinceLastLogin, data=d0)
length(model_sbc$residuals)
length(model_sbc2$residuals)
```

Le variabili che presentano NA values sono cambiate, il nuovo modello è stato stimato su un numero di osservazioni maggiore (15 in più), poichè abbiamo rimosso la variabile *productsBought*.

Una volta svolta la model selection, ci preoccupiamo dei valori influenti.

## Valori influenti
Osservazioni inusuali all'interno del dataset possono risultare problematiche quando si vuole stimare un modello di regressione lineare tramite stimatori a minimi quadrati. Infatti, questi valori possono influire sui risultati dell'analisi e dunque vanno rimossi.  
Per identificare queste particolari osservazioni, chimate valori influenti, ci serviamo di un Influence Plot che dispone le osservazioni su un grafico con in ascissa i valori degli hat-values e in ordinata i valori dei residui studentizzati.
```{r}
cooksd <- cooks.distance(model_sbc2)
library(car)
influencePlot(model_sbc2, main = "Influence plot")
```

Dalla dimensione delle bolle nel grafico identifichiamo alcuni dei valori influenti nel nostro dataset. 
Il venditore 9 risulta avere un alto hat-value ma un basso residuo studentizzato: questo venditore ha un elevato numero di followers iscritti al suo canale di user's activity, è un venditore che ha messo un gran numero di 'mi piace' a prodotti presenti sul social network, ma il numero di prodotti che vende non è significativamente maggiore rispetto ad altri venditori che sono molto meno attivi sui social.
Invece i venditori 159 e 22 hanno un valore non particolarmente elevato di hat-value, ma il loro residuo studentizzato è alto: questi venditori riescono a vendere un numero di prodotti molto più alto rispetto ad altri venditori che hanno simili caratteristiche, come i numeri di followers, 'mi piace' e prodotti in lista.

Grazie alle distanze di cook verifichiamo l'eventuale influenza di questi venditori sui parametri del modello e sui valori previsti. Se ciò si verifica, allora dovremo escluderli dal dataset.
```{r}
soglia <- 4/(length(model_sbc2$residuals)-length(model_sbc2$coefficients)-2)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance") 
abline(h = soglia, col="red")
```

Il grafico mostra con gli asterischi le osservazioni che presentano distanze di cook che superano la soglia 4/n-p-2 (molto prossima a 0) e procediamo con la loro rimozione dal dataset.
```{r}
influential <- as.numeric(names(cooksd)[cooksd > soglia])
data_finale <- d0[-influential,]
data_finale[c("9","22","159"),]
```

Precisamente eliminiamo 67 osservazioni (tra cui il venditore 9 e 159). 

Ora confrontiamo il modello imputato sul dataset senza valori influenti e il modello imputato sul dataset completo.
```{r}
model_final = lm((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked + 
                productsListed + productsPassRate + I(productsPassRate^2) + 
                I(productsPassRate^3) + daysSinceLastLogin, data=data_finale)
summary(model_sbc2)
summary(model_final)
```

E' da osservare che i parametri del modello imputato sul detaset senza valori influenti cambiano (anche se non di molto) rispetto ai parametri del modello precedente. L'adattamento migliora: l'R^2 aggiustato aumenta da 0.78 a 0.82.

Il modello finale stimato, qualsiasi siano gli step e trasformazioni adottate sulle variabili, resta sempre un modello lineare. 
Il nostro obiettivo è quello di massimizzare la correlazione tra i valori osservati della variabile dipendente (*productSold*) e i valori previsti del modello, per questo motivo ricaviamo i grafici 'y vs f(y)' per il modello finale e per quello con i valori influenti, così da poterli confrontare.
```{r}
d1<-subset(data_finale, (!is.na(data_finale[,"productsListed"])))
b = data.frame((d0_nona$productsSold)^(-0.5), model_sbc$fitted.values)
prova1<- lm((d0_nona$productsSold)^(-0.5) ~ model_sbc$fitted.values, data=b)
plot((d0_nona$productsSold)^(-0.5), model_sbc$fitted.values, ylim = c(0,1))
abline(prova1, col = 'red')
```

```{r}
a = data.frame((d1$productsSold)^(-0.5), model_final$fitted.values)
prova <- lm((d1$productsSold)^(-0.5) ~ model_final$fitted.values, data=a)
plot((d1$productsSold)^(-0.5), model_final$fitted.values, ylim = c(0,1))
abline(prova, col = 'red')
```

Nel grafico del modello finale osserviamo che la correlazione tra y e y fittato sembra essersi rafforzata, infatti nel secondo grafico i punti sono meno dispersi attorno alla retta poichè abbiamo tolto le osservazioni influenti sul modello.

```{r}
par(mfrow=c(2,2)) 
plot(model_final)
par(mfrow=c(1,1))
```

Un notevole cambiamento si verifica nel grafico in basso a destra, in quanto non sono più presenti valori che oltrepassano la linea rossa della distanza di Cook. 
La presenza di outliers e valori influenti è spesso anche fonte di eteroschedasticità, dunque, nel momento in cui questi vengono rimossi dal dataset, questo problema si attenua. Tuttavia, dai grafici questo miglioramento non appare evidente, dunque ci serviamo di test statistici più precisi.

# ETEROSCHEDASTICITA'
La presenza di eteroschedasticità implica la violazione sull'assunzione di variabilità costante degli errori del modello stimato. Tale condizione comporta diverse difficoltà: gli stimatori a minimi quadrati in presenza di eteroschedasticità restano comunque stimatori lineari e corretti ma, non sono più i più efficienti, ovvero, esistono altri stimatori con varianza minore. Un altro problema, di maggior interesse, è la stima errata degli stimatori degli stadard error, su cui è basata l'inferenza. 
Per capire se il nostro modello finale soffre di questo problema eseguiamo i test di White e Breush-Pagan su più modelli.
```{r}
library(lmtest)
bptest(starting_model)
bptest(model_5)
bptest(model_final)
library(car)
ncvTest(starting_model)
ncvTest(model_5)
ncvTest(model_final)
```

Il modello finale soffre di eteroschedasticità, poichè rifiutiamo l'ipotesi nulla H0 di varianza costante dei residui elaborata da White. Tuttavia, è da sottolineare il miglioramento della statistica test chi-quadrato, da 27.88 a 8.4, ottenuto passando rispettivamente dal modello stimato sul dataset completo (model_5) al modello stimato sul dataset senza valori influenti (model_final).

Procediamo alla stima degli standard error robusti di White, così da poter svolgere un'inferenza corretta sui nostri parametri.
```{r}
library('lmSupport')
modelCorrectSE(model_final)
```

Notiamo che gli standard error robusti di white non sono particolarmente diversi dagli standard error non robusti stimati dal modello, infatti, il nostro modello non soffre di una pesante eteroschedasticità.
Si osserva che le nuove statistiche t associate alle variabili, sono mediamente più piccole in modulo rispetto alle statistiche t stimate senza SE robusti. 

Valutiamo graficamente quanto appena osservato:
```{r}
library('lmtest')
library('sandwich')
cf=as.matrix(coeftest(model_final, vcov=vcovHC(model_final)))
plot(cf[,4], summary(model_final)$coefficients[,4] , asp = 1)
abline(a = 0, b = 1, col = 2)
```

In ascissa sono riportati i valori dei p-value corretti e in ordinata i valori dei p-value calcolati sui parametri del modello. Rispetto alla bisettrice del grafico, la maggior parte di questi è posizionata lungo la linea (sovrapposti l'un con l'altro), eccetto uno che si discosta leggermente. Tale discostamento è sinonimo di eteroschedasticità, a conferma di quanto già considerato.

## Bootstrap

Valutiamo la robustezza del modello finale tramite la strategia Bootstrap sui parametri.
```{r}
library("car")
model_final = lm((productsSold)^(-0.5) ~ socialNbFollowers + socialProductsLiked + 
                   productsListed + productsPassRate + I(productsPassRate^2) + I(productsPassRate^3) + 
                   daysSinceLastLogin, data=d1)
BOOT.MOD = Boot(model_final, R=1999)
summary(BOOT.MOD, high.moments=TRUE)
```

```{r}
Confint(BOOT.MOD, level=c(.95), type="perc")
hist(BOOT.MOD, legend="separate")
```

Da questi grafici possiamo osservare che i parametri del nostro modello finale sono robusti, infatti, per ogni variabile l'intervallo di confidenza Boot (empirico) è centrato sulla stima del rispettivo parametro. 
Intuiamo, quindi, che queste stime non sovrastimano o sottostimano i vari effetti delle covariate sulla variabile dipendente (*productsSold*). L'intervallo di confidenza Boot della variabile productsPassRate comprende il valore 0, quindi possiamo concludere che questa variabile non è significativa all'interno del nostro modello. 

Ricalcoliamo gli intervalli anche secondo il normal boot CI
```{r}
Confint(BOOT.MOD, level=c(.95), type="norm")
```
Questo output conferma il precedente. 





